[paths]
train = null
dev = null
vectors = "training/md/lemma/model-best"
init_tok2vec = null

[system]
gpu_allocator = null
seed = 0

[nlp]
lang = "la"
pipeline = ["senter","normer","tok2vec","tagger","morphologizer","trainable_lemmatizer","parser","lookup_lemmatizer","ner"]
tokenizer = {"@tokenizers":"spacy.Tokenizer.v1"}
disabled = ["senter"]
before_creation = null
after_creation = null
after_pipeline_creation = null
batch_size = 1000

[components]

[components.senter]
source = "training/md/lemma/model-best"

[components.normer]
factory = "normer"

[components.tok2vec]
source = "training/md/lemma/model-best"
include_static_vectors = true

[components.tagger]
source = "training/md/lemma/model-best"

[components.morphologizer]
source = "training/md/lemma/model-best"

[components.trainable_lemmatizer]
source = "training/md/lemma/model-best"

[components.parser]
source = "training/md/lemma/model-best"

[components.lookup_lemmatizer]
factory = "lookup_lemmatizer"

[components.ner]
source = "training/md/ner/model-best"
replace_listeners = ["model.tok2vec"]

[pretraining]

[initialize]
vectors = ${paths.vectors}
init_tok2vec = ${paths.init_tok2vec}
vocab_data = null
lookups = null
before_init = {"@callbacks":"customize_tokenizer"}
after_init = null

[initialize.components]

[initialize.tokenizer]
