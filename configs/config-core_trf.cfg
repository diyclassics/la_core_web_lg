[paths]
train = null
dev = null
vectors = null
init_tok2vec = null

[system]
gpu_allocator = "pytorch"
seed = 0

[nlp]
lang = "la"
pipeline = ["senter","transformer","normer","tagger","morphologizer","trainable_lemmatizer","parser","lookup_lemmatizer","ner"]
tokenizer = {"@tokenizers":"spacy.Tokenizer.v1"}
disabled = []
before_creation = null
after_creation = null
after_pipeline_creation = null
batch_size = 1000

[components]

[components.senter]
source = "training/trf/lemma/model-best"
replace_listeners = ["model.tok2vec"]

[components.normer]
factory = "normer"

[components.transformer]
source = "training/trf/lemma/model-best"

[components.tagger]
source = "training/trf/lemma/model-best"

[components.morphologizer]
source = "training/trf/lemma/model-best"

[components.trainable_lemmatizer]
source = "training/trf/lemma/model-best"

[components.parser]
source = "training/trf/lemma/model-best"

[components.lookup_lemmatizer]
factory = "lookup_lemmatizer"

[components.ner]
source = "training/trf/ner/model-best"
replace_listeners = ["model.tok2vec"]

[pretraining]

[initialize]
vectors = ${paths.vectors}
init_tok2vec = ${paths.init_tok2vec}
vocab_data = null
lookups = null
before_init = {"@callbacks":"customize_tokenizer"}
after_init = null

[initialize.components]

[initialize.tokenizer]
